from datetime import datetime, timedelta
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
from apscheduler.executors.pool import ThreadPoolExecutor
from apscheduler.triggers.date import DateTrigger
from apscheduler.triggers.cron import CronTrigger
from models import NotifyTask, NotifyStatus, ExternalCalendar, UserChannel, get_db, DATABASE_URL
from notifier import NotificationSender, parse_config
from hooks import execute_hook
import logging
import queue
import requests
import re
import traceback
import socket
import os
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EventManager:
    def __init__(self):
        # listeners: list of (queue, user_id)
        self.listeners = []

    def listen(self, user_id):
        q = queue.Queue(maxsize=10)
        self.listeners.append((q, user_id))
        return q

    def announce(self, user_id, msg):
        for i in range(len(self.listeners) - 1, -1, -1):
            q, uid = self.listeners[i]
            if uid == user_id:
                try:
                    q.put_nowait(msg)
                except queue.Full:
                    del self.listeners[i]

event_manager = EventManager()


def get_cron_trigger(expression):
    """æ ¹æ® cron è¡¨è¾¾å¼è·å–è§¦å‘å™¨ï¼Œæ”¯æŒ 5 ä½ (åˆ†æ—¶æ—¥æœˆå‘¨) å’Œ 6 ä½ (ç§’åˆ†æ—¶æ—¥æœˆå‘¨)"""
    values = expression.strip().split()
    if len(values) == 6:
        return CronTrigger(
            second=values[0],
            minute=values[1],
            hour=values[2],
            day=values[3],
            month=values[4],
            day_of_week=values[5]
        )
    return CronTrigger.from_crontab(expression)


# ============ æ¨¡å—çº§å‡½æ•°ï¼ˆç”¨äº APScheduler jobï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜ï¼‰ ============

def execute_task(task_id: int):
    """
    æ‰§è¡Œé€šçŸ¥ä»»åŠ¡ï¼ˆæ¨¡å—çº§å‡½æ•°ï¼Œå¯è¢«åºåˆ—åŒ–ï¼‰
    
    Args:
        task_id: ä»»åŠ¡ID
    """
    from models import TaskExecutionLog
    
    execution_start = datetime.now()
    worker_id = f"{os.getpid()}"
    hostname = socket.gethostname()
    job_id = None
    log = None
    
    with get_db() as db:
        try:
            # ä½¿ç”¨æ•°æ®åº“è¡Œçº§é”è·å–ä»»åŠ¡ï¼Œé˜²æ­¢å¹¶å‘æ‰§è¡Œï¼ˆæ‚²è§‚é”ï¼‰
            # with_for_update() ä¼šåœ¨å½“å‰äº‹åŠ¡ä¸­é”å®šè¯¥è¡Œï¼Œç›´åˆ°äº‹åŠ¡æäº¤æˆ–å›æ»š
            task = db.query(NotifyTask).filter(NotifyTask.id == task_id).with_for_update().first()
            if not task:
                logger.error(f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
                return

            job_id = f"recurring_task_{task_id}" if task.is_recurring else f"task_{task_id}"
            
            # ç”Ÿæˆé‡å¤æ£€æµ‹é”®ï¼ˆç²¾ç¡®åˆ°ç§’ï¼‰
            duplicate_check_key = f"{task_id}_{execution_start.strftime('%Y%m%d%H%M%S')}"
            
            # é‡å¤æ‰§è¡Œæ£€æµ‹ - åœ¨åŒä¸€äº‹åŠ¡ä¸­æ£€æŸ¥æœ€è¿‘3ç§’å†…æ˜¯å¦æœ‰ç›¸åŒä»»åŠ¡æ‰§è¡Œ
            # ç”±äºä½¿ç”¨äº†è¡Œé”ï¼Œè¿™é‡Œçš„æ£€æŸ¥æ›´å¯é ï¼Œé¿å…ç«æ€æ¡ä»¶
            recent_logs = db.query(TaskExecutionLog).filter(
                TaskExecutionLog.task_id == task_id,
                TaskExecutionLog.execution_start >= execution_start - timedelta(seconds=3),
                TaskExecutionLog.status.in_(['started', 'success'])
            ).all()
            
            is_duplicate = len(recent_logs) > 0
            if is_duplicate:
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ°ä»»åŠ¡ {task_id} é‡å¤æ‰§è¡Œï¼ˆæœ€è¿‘3ç§’å†…å·²æ‰§è¡Œ {len(recent_logs)} æ¬¡ï¼‰ï¼Œæœ¬æ¬¡è·³è¿‡")
                
                # è®°å½•é‡å¤æ‰§è¡Œæ—¥å¿—ï¼ˆç”¨äºç›‘æ§ç»Ÿè®¡ï¼‰
                duplicate_log = TaskExecutionLog(
                    task_id=task_id,
                    job_id=job_id,
                    execution_start=execution_start,
                    execution_end=datetime.now(),
                    execution_duration=0,
                    status='skipped',
                    result_summary='æ£€æµ‹åˆ°é‡å¤æ‰§è¡Œï¼Œå·²è·³è¿‡',
                    worker_id=worker_id,
                    hostname=hostname,
                    duplicate_check_key=duplicate_check_key,
                    is_duplicate=True
                )
                db.add(duplicate_log)
                db.commit()
                
                # è§¦å‘å‘Šè­¦
                try:
                    check_and_alert(task_id, 'duplicate_execution', db)
                except Exception as alert_err:
                    logger.error(f"è§¦å‘å‘Šè­¦å¤±è´¥: {str(alert_err)}")
                return

            # å¦‚æœä»»åŠ¡å·²å–æ¶ˆï¼Œè·³è¿‡æ‰§è¡Œ
            if task.status == NotifyStatus.CANCELLED:
                logger.info(f"ä»»åŠ¡ {task_id} å·²å–æ¶ˆï¼Œè·³è¿‡æ‰§è¡Œ")
                return

            # æ£€æŸ¥æš‚åœçŠ¶æ€
            if task.status == NotifyStatus.PAUSED:
                logger.info(f"ä»»åŠ¡ {task_id} å·²æš‚åœï¼Œè·³è¿‡æ‰§è¡Œ")
                return
            
            # è®°å½•å¼€å§‹æ‰§è¡Œ
            log = TaskExecutionLog(
                task_id=task_id,
                job_id=job_id,
                execution_start=execution_start,
                status='started',
                worker_id=worker_id,
                hostname=hostname,
                duplicate_check_key=duplicate_check_key
            )
            db.add(log)
            db.commit()

            logger.info(f"âœ“ å¼€å§‹æ‰§è¡Œä»»åŠ¡ {task_id}: {task.title} (Worker: {worker_id})")

            # æ‰§è¡Œ before_execute é’©å­
            try:
                hook_result = execute_hook(
                    hook_type='before_execute',
                    task_id=task_id,
                    task=task,
                    context={},
                    db_session=db,
                    task_execution_log_id=log.id
                )
                if not hook_result.get('success') and not hook_result.get('skipped'):
                    logger.warning(f"before_execute é’©å­æ‰§è¡Œå¤±è´¥: {hook_result.get('error')}")
            except Exception as hook_err:
                logger.error(f"æ‰§è¡Œ before_execute é’©å­æ—¶å‡ºé”™: {str(hook_err)}")

            # æ£€æµ‹æ˜¯å¤šæ¸ é“è¿˜æ˜¯å•æ¸ é“ä»»åŠ¡
            is_multi_channel = task.channels_json is not None
            
            if is_multi_channel:
                # å¤šæ¸ é“æ¨¡å¼
                try:
                    channels = json.loads(task.channels_json)
                    channels_config = json.loads(task.channels_config_json)
                except (json.JSONDecodeError, TypeError) as e:
                    raise Exception(f"é…ç½®è§£æå¤±è´¥: {str(e)}")
                
                send_results = {}
                success_count = 0
                fail_count = 0
                
                # éå†æ‰€æœ‰æ¸ é“å‘é€é€šçŸ¥
                for channel_str in channels:
                    try:
                        from models import NotifyChannel
                        channel = NotifyChannel(channel_str)
                        config = parse_config(channels_config.get(channel_str, {}))
                        
                        logger.info(f"ä»»åŠ¡ {task_id} å‘æ¸ é“ {channel_str} å‘é€é€šçŸ¥")
                        NotificationSender.send(
                            channel=channel,
                            config=config,
                            title=task.title,
                            content=task.content
                        )
                        
                        send_results[channel_str] = {
                            'status': 'sent',
                            'message': 'å‘é€æˆåŠŸ',
                            'sent_time': datetime.now().isoformat()
                        }
                        success_count += 1
                        logger.info(f"ä»»åŠ¡ {task_id} æ¸ é“ {channel_str} å‘é€æˆåŠŸ")
                        
                    except Exception as e:
                        send_results[channel_str] = {
                            'status': 'failed',
                            'message': str(e),
                            'sent_time': datetime.now().isoformat()
                        }
                        fail_count += 1
                        logger.error(f"ä»»åŠ¡ {task_id} æ¸ é“ {channel_str} å‘é€å¤±è´¥: {str(e)}")
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                if not task.is_recurring:
                    if success_count > 0:
                        task.status = NotifyStatus.SENT
                    else:
                        task.status = NotifyStatus.FAILED
                
                task.sent_time = datetime.now()
                task.send_results = json.dumps(send_results, ensure_ascii=False)
                
                # è®¾ç½®é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰å¤±è´¥ï¼‰
                if fail_count > 0:
                    task.error_msg = f"{success_count}/{len(channels)} ä¸ªæ¸ é“å‘é€æˆåŠŸï¼Œ{fail_count} ä¸ªå¤±è´¥"
                else:
                    task.error_msg = None
                
                # é‡å¤ä»»åŠ¡æ‰§è¡ŒæˆåŠŸåï¼Œæ»šåŠ¨æ›´æ–°ä¸‹ä¸€æ¬¡æ‰§è¡Œæ—¶é—´
                if task.is_recurring and task.cron_expression:
                    try:
                        trigger = get_cron_trigger(task.cron_expression)
                        base_time = datetime.now()
                        next_run = trigger.get_next_fire_time(None, base_time)
                        if next_run:
                            task.scheduled_time = next_run
                    except Exception as e:
                        logger.warning(f"ä»»åŠ¡ {task_id} æ›´æ–°ä¸‹ä¸€æ¬¡æ‰§è¡Œæ—¶é—´å¤±è´¥: {str(e)}")
                
                # æ›´æ–°æ‰§è¡Œæ—¥å¿—
                execution_end = datetime.now()
                duration = (execution_end - execution_start).total_seconds()
                log.execution_end = execution_end
                log.execution_duration = duration
                log.status = 'success' if success_count > 0 else 'failed'
                log.result_summary = f"å¤šæ¸ é“æ‰§è¡Œå®Œæˆ: {success_count} æˆåŠŸ, {fail_count} å¤±è´¥"
                log.channel_results = json.dumps(send_results, ensure_ascii=False)
                log.success_count = success_count
                log.failed_count = fail_count
                
                logger.info(f"âœ“ ä»»åŠ¡ {task_id} å¤šæ¸ é“æ‰§è¡Œå®Œæˆ: {success_count} æˆåŠŸ, {fail_count} å¤±è´¥ï¼Œè€—æ—¶ {duration:.2f}ç§’")
                
                # æ‰§è¡Œ after_success é’©å­
                if success_count > 0:
                    try:
                        hook_result = execute_hook(
                            hook_type='after_success',
                            task_id=task_id,
                            task=task,
                            context={'send_results': send_results},
                            db_session=db,
                            task_execution_log_id=log.id
                        )
                        if not hook_result.get('success') and not hook_result.get('skipped'):
                            logger.warning(f"after_success é’©å­æ‰§è¡Œå¤±è´¥: {hook_result.get('error')}")
                    except Exception as hook_err:
                        logger.error(f"æ‰§è¡Œ after_success é’©å­æ—¶å‡ºé”™: {str(hook_err)}")
                
                # é€šçŸ¥å‰ç«¯
                event_manager.announce(task.user_id, {
                    'type': 'task_executed',
                    'task_id': task.id,
                    'title': task.title,
                    'status': 'sent' if success_count > 0 else 'failed',
                    'message': f'{success_count}/{len(channels)} ä¸ªæ¸ é“å‘é€æˆåŠŸ'
                })
                
            else:
                # å•æ¸ é“æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                config = parse_config(task.channel_config)

                # å‘é€é€šçŸ¥
                NotificationSender.send(
                    channel=task.channel,
                    config=config,
                    title=task.title,
                    content=task.content
                )

                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                if not task.is_recurring:
                    task.status = NotifyStatus.SENT
                task.sent_time = datetime.now()
                task.error_msg = None

                # é‡å¤ä»»åŠ¡æ‰§è¡ŒæˆåŠŸåï¼Œæ»šåŠ¨æ›´æ–°ä¸‹ä¸€æ¬¡æ‰§è¡Œæ—¶é—´
                if task.is_recurring and task.cron_expression:
                    try:
                        trigger = get_cron_trigger(task.cron_expression)
                        base_time = datetime.now()
                        next_run = trigger.get_next_fire_time(None, base_time)
                        if next_run:
                            task.scheduled_time = next_run
                    except Exception as e:
                        logger.warning(f"ä»»åŠ¡ {task_id} æ›´æ–°ä¸‹ä¸€æ¬¡æ‰§è¡Œæ—¶é—´å¤±è´¥: {str(e)}")
                
                # æ›´æ–°æ‰§è¡Œæ—¥å¿—
                execution_end = datetime.now()
                duration = (execution_end - execution_start).total_seconds()
                log.execution_end = execution_end
                log.execution_duration = duration
                log.status = 'success'
                log.result_summary = 'å•æ¸ é“å‘é€æˆåŠŸ'
                log.success_count = 1
                log.failed_count = 0

                logger.info(f"âœ“ ä»»åŠ¡ {task_id} æ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶ {duration:.2f}ç§’")
                
                # æ‰§è¡Œ after_success é’©å­
                try:
                    hook_result = execute_hook(
                        hook_type='after_success',
                        task_id=task_id,
                        task=task,
                        context={'send_results': {'single_channel': {'status': 'sent'}}},
                        db_session=db,
                        task_execution_log_id=log.id
                    )
                    if not hook_result.get('success') and not hook_result.get('skipped'):
                        logger.warning(f"after_success é’©å­æ‰§è¡Œå¤±è´¥: {hook_result.get('error')}")
                except Exception as hook_err:
                    logger.error(f"æ‰§è¡Œ after_success é’©å­æ—¶å‡ºé”™: {str(hook_err)}")
                
                # é€šçŸ¥å‰ç«¯
                event_manager.announce(task.user_id, {
                    'type': 'task_executed',
                    'task_id': task.id,
                    'title': task.title,
                    'status': 'sent',
                    'message': 'å‘é€æˆåŠŸ'
                })

            db.commit()

        except Exception as e:
            # æ‰§è¡Œå¤±è´¥
            execution_end = datetime.now()
            duration = (execution_end - execution_start).total_seconds() if execution_start else 0
            
            if log:
                log.execution_end = execution_end
                log.execution_duration = duration
                log.status = 'failed'
                log.error_message = str(e)
                log.error_traceback = traceback.format_exc()
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            try:
                task = db.query(NotifyTask).filter(NotifyTask.id == task_id).first()
                if task:
                    task.status = NotifyStatus.FAILED
                    task.error_msg = str(e)
                    
                    # é€šçŸ¥å‰ç«¯
                    event_manager.announce(task.user_id, {
                        'type': 'task_executed',
                        'task_id': task.id,
                        'title': task.title,
                        'status': 'failed',
                        'message': str(e)
                    })
            except:
                pass
            
            db.commit()
            logger.error(f"âœ— ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {str(e)}\n{traceback.format_exc()}")
            
            # æ‰§è¡Œ after_failure é’©å­
            try:
                task_obj = db.query(NotifyTask).filter(NotifyTask.id == task_id).first()
                if task_obj:
                    hook_result = execute_hook(
                        hook_type='after_failure',
                        task_id=task_id,
                        task=task_obj,
                        context={'error': str(e), 'traceback': traceback.format_exc()},
                        db_session=db,
                        task_execution_log_id=log.id if log else None
                    )
                    if not hook_result.get('success') and not hook_result.get('skipped'):
                        logger.warning(f"after_failure é’©å­æ‰§è¡Œå¤±è´¥: {hook_result.get('error')}")
            except Exception as hook_err:
                logger.error(f"æ‰§è¡Œ after_failure é’©å­æ—¶å‡ºé”™: {str(hook_err)}")
            
            # è§¦å‘å‘Šè­¦
            try:
                check_and_alert(task_id, 'execution_failure', db)
            except Exception as alert_err:
                logger.error(f"è§¦å‘å‘Šè­¦å¤±è´¥: {str(alert_err)}")


def check_and_alert(task_id: int, alert_type: str, db_session):
    """
    æ£€æŸ¥å¹¶è§¦å‘å‘Šè­¦ï¼ˆæ¨¡å—çº§å‡½æ•°ï¼‰
    
    Args:
        task_id: ä»»åŠ¡ID
        alert_type: å‘Šè­¦ç±»å‹ (duplicate_execution, execution_failure)
        db_session: æ•°æ®åº“ä¼šè¯
    """
    try:
        from models import AlertRule, TaskExecutionLog
        
        # æŸ¥è¯¢ä»»åŠ¡å’Œç”¨æˆ·çš„å‘Šè­¦è§„åˆ™
        task = db_session.query(NotifyTask).filter(NotifyTask.id == task_id).first()
        if not task:
            return
        
        rules = db_session.query(AlertRule).filter(
            AlertRule.user_id == task.user_id,
            AlertRule.rule_type == alert_type,
            AlertRule.is_enabled == True
        ).all()
        
        for rule in rules:
            params = json.loads(rule.rule_params) if rule.rule_params else {}
            
            if alert_type == 'duplicate_execution':
                # æ£€æŸ¥æ—¶é—´çª—å£å†…çš„é‡å¤æ‰§è¡Œæ¬¡æ•°
                time_window = params.get('time_window', 300)  # é»˜è®¤5åˆ†é’Ÿ
                threshold = params.get('threshold', 2)
                
                # ç»Ÿè®¡é‡å¤æ‰§è¡Œæ¬¡æ•°ï¼ˆstatus='skipped' ä¸” is_duplicate=Trueï¼‰
                recent_duplicates = db_session.query(TaskExecutionLog).filter(
                    TaskExecutionLog.task_id == task_id,
                    TaskExecutionLog.is_duplicate == True,
                    TaskExecutionLog.execution_start >= datetime.now() - timedelta(seconds=time_window)
                ).count()
                
                if recent_duplicates >= threshold:
                    logger.warning(f"ğŸ“Š å‘Šè­¦è§¦å‘: ä»»åŠ¡ {task_id} åœ¨{time_window}ç§’å†…æ£€æµ‹åˆ°{recent_duplicates}æ¬¡é‡å¤æ‰§è¡Œ")
                    send_alert(rule, task, f"ä»»åŠ¡ '{task.title}' åœ¨{time_window}ç§’å†…æ£€æµ‹åˆ°{recent_duplicates}æ¬¡é‡å¤æ‰§è¡Œï¼Œå¯èƒ½å­˜åœ¨å¤šè¿›ç¨‹è°ƒåº¦é—®é¢˜", db_session)
            
            elif alert_type == 'execution_failure':
                # æ£€æŸ¥å¤±è´¥ç‡
                time_window = params.get('time_window', 3600)  # é»˜è®¤1å°æ—¶
                failure_threshold = params.get('threshold', 3)
                
                recent_failures = db_session.query(TaskExecutionLog).filter(
                    TaskExecutionLog.task_id == task_id,
                    TaskExecutionLog.status == 'failed',
                    TaskExecutionLog.execution_start >= datetime.now() - timedelta(seconds=time_window)
                ).count()
                
                if recent_failures >= failure_threshold:
                    send_alert(rule, task, f"ä»»åŠ¡åœ¨{time_window}ç§’å†…å¤±è´¥{recent_failures}æ¬¡", db_session)
    
    except Exception as e:
        logger.error(f"å‘Šè­¦æ£€æŸ¥å¤±è´¥: {str(e)}")


def send_alert(rule, task: NotifyTask, message: str, db_session):
    """
    å‘é€å‘Šè­¦é€šçŸ¥ï¼ˆæ¨¡å—çº§å‡½æ•°ï¼‰
    
    Args:
        rule: å‘Šè­¦è§„åˆ™å¯¹è±¡
        task: ä»»åŠ¡å¯¹è±¡
        message: å‘Šè­¦æ¶ˆæ¯
        db_session: æ•°æ®åº“ä¼šè¯
    """
    try:
        if not rule.alert_channel:
            logger.warning(f"å‘Šè­¦è§„åˆ™ {rule.id} æœªé…ç½®å‘Šè­¦æ¸ é“")
            return
        
        config = parse_config(rule.alert_channel.channel_config)
        title = f"âš ï¸ ä»»åŠ¡å‘Šè­¦: {task.title}"
        content = f"""
ä»»åŠ¡ID: {task.id}
ä»»åŠ¡åç§°: {task.title}
å‘Šè­¦ç±»å‹: {rule.rule_type}
å‘Šè­¦ä¿¡æ¯: {message}
å‘Šè­¦æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

è¯·åŠæ—¶æ£€æŸ¥ä»»åŠ¡çŠ¶æ€ã€‚
"""
        
        NotificationSender.send(
            channel=rule.alert_channel.channel_type,
            config=config,
            title=title,
            content=content
        )
        logger.info(f"âœ“ å‘Šè­¦é€šçŸ¥å·²å‘é€: {message}")
    except Exception as e:
        logger.error(f"âœ— å‘é€å‘Šè­¦å¤±è´¥: {str(e)}")


class NotifyScheduler:
    """é€šçŸ¥è°ƒåº¦å™¨"""
    
    def __init__(self):
        # é…ç½® jobstore - ä½¿ç”¨ SQLAlchemy æŒä¹…åŒ–å­˜å‚¨
        jobstores = {
            'default': SQLAlchemyJobStore(url=DATABASE_URL, tablename='apscheduler_jobs')
        }
        
        # é…ç½®æ‰§è¡Œå™¨
        executors = {
            'default': ThreadPoolExecutor(max_workers=10)
        }
        
        # è°ƒåº¦å™¨å…¨å±€é…ç½® - é˜²æ­¢ä»»åŠ¡é‡å¤æ‰§è¡Œçš„å…³é”®é…ç½®
        job_defaults = {
            'coalesce': True,           # åˆå¹¶é”™è¿‡çš„æ‰§è¡Œï¼Œåªæ‰§è¡Œæœ€æ–°ä¸€æ¬¡
            'max_instances': 1,         # æ¯ä¸ªä»»åŠ¡æœ€å¤šåŒæ—¶è¿è¡Œ1ä¸ªå®ä¾‹ï¼ˆé˜²é‡å¤å…³é”®ï¼‰
            'misfire_grace_time': 60    # é”™è¿‡æ—¶é—´å®¹å¿åº¦60ç§’
        }
        
        self.scheduler = BackgroundScheduler(
            jobstores=jobstores,
            executors=executors,
            job_defaults=job_defaults,
            timezone='Asia/Shanghai'
        )
        self.scheduler.start()
        logger.info("é€šçŸ¥è°ƒåº¦å™¨å·²å¯åŠ¨ï¼ˆä½¿ç”¨SQLAlchemyJobStoreé˜²é‡å¤æ‰§è¡Œï¼‰")
    
    def add_task(self, task: NotifyTask):
        """
        æ·»åŠ é€šçŸ¥ä»»åŠ¡åˆ°è°ƒåº¦å™¨
        
        Args:
            task: é€šçŸ¥ä»»åŠ¡å¯¹è±¡
        """
        if task.is_recurring and task.cron_expression:
            # é‡å¤ä»»åŠ¡ï¼Œä½¿ç”¨ cron è¡¨è¾¾å¼
            try:
                trigger = get_cron_trigger(task.cron_expression)
                job_id = f"recurring_task_{task.id}"
                
                self.scheduler.add_job(
                    func=execute_task,
                    trigger=trigger,
                    args=[task.id],
                    id=job_id,
                    replace_existing=True,
                    coalesce=True,
                    max_instances=1,
                    misfire_grace_time=60
                )
                logger.info(f"ä»»åŠ¡ {task.id} å·²æ·»åŠ åˆ°è°ƒåº¦å™¨ï¼Œè®¡åˆ’æ‰§è¡Œæ—¶é—´: {task.scheduled_time}")
            except Exception as e:
                logger.error(f"æ·»åŠ ä»»åŠ¡ {task.id} å¤±è´¥ï¼ŒCron è¡¨è¾¾å¼æ— æ•ˆ: {e}")
        else:
            # ä¸€æ¬¡æ€§ä»»åŠ¡ï¼Œä½¿ç”¨æŒ‡å®šæ—¶é—´
            trigger = DateTrigger(run_date=task.scheduled_time)
            job_id = f"task_{task.id}"
        
            self.scheduler.add_job(
                func=execute_task,
                trigger=trigger,
                args=[task.id],
                id=job_id,
                replace_existing=True,
                coalesce=True,
                max_instances=1,
                misfire_grace_time=60
            )
            
            logger.info(f"ä»»åŠ¡ {task.id} å·²æ·»åŠ åˆ°è°ƒåº¦å™¨ï¼Œè®¡åˆ’æ‰§è¡Œæ—¶é—´: {task.scheduled_time}")
    
    def remove_task(self, task_id: int, is_recurring: bool = False):
        """
        ä»è°ƒåº¦å™¨ä¸­ç§»é™¤ä»»åŠ¡
        
        Args:
            task_id: ä»»åŠ¡ID
            is_recurring: æ˜¯å¦ä¸ºé‡å¤ä»»åŠ¡
        """
        try:
            job_id = f"recurring_task_{task_id}" if is_recurring else f"task_{task_id}"
            self.scheduler.remove_job(job_id)
            logger.info(f"ä»»åŠ¡ {task_id} å·²ä»è°ƒåº¦å™¨ç§»é™¤")
        except Exception as e:
            logger.warning(f"ç§»é™¤ä»»åŠ¡ {task_id} å¤±è´¥: {str(e)}")
    
    def load_pending_tasks(self):
        """
        åŠ è½½æ‰€æœ‰å¾…å‘é€çš„ä»»åŠ¡åˆ°è°ƒåº¦å™¨
        """
        with get_db() as db:
            try:
                # æŸ¥è¯¢æ‰€æœ‰å¾…å‘é€çš„ä»»åŠ¡
                pending_tasks = db.query(NotifyTask).filter(
                    NotifyTask.status == NotifyStatus.PENDING
                ).all()

                logger.info(f"æ‰¾åˆ° {len(pending_tasks)} ä¸ªå¾…å‘é€ä»»åŠ¡")

                for task in pending_tasks:
                    try:
                        # éªŒè¯ä»»åŠ¡é…ç½®ï¼šå•æ¸ é“ä»»åŠ¡å¿…é¡»æœ‰channelï¼Œå¤šæ¸ é“ä»»åŠ¡å¿…é¡»æœ‰channels_json
                        is_multi_channel = task.channels_json is not None
                        if not is_multi_channel and task.channel is None:
                            logger.warning(f"ä»»åŠ¡ {task.id} é…ç½®æ— æ•ˆï¼ˆå•æ¸ é“å’Œå¤šæ¸ é“å­—æ®µéƒ½ä¸ºç©ºï¼‰ï¼Œè·³è¿‡åŠ è½½")
                            continue
                        
                        # å¦‚æœæ˜¯ä¸€æ¬¡æ€§ä»»åŠ¡ä¸”è®¡åˆ’æ—¶é—´å·²è¿‡ï¼Œè·³è¿‡
                        if not task.is_recurring and task.scheduled_time < datetime.now():
                            logger.warning(f"ä»»åŠ¡ {task.id} è®¡åˆ’æ—¶é—´å·²è¿‡ï¼Œè·³è¿‡åŠ è½½")
                            continue

                        # å¦‚æœæ˜¯é‡å¤ä»»åŠ¡ä¸”è®¡åˆ’æ—¶é—´å·²è¿‡ï¼Œé‡æ–°è®¡ç®—ä¸‹ä¸€æ¬¡æ‰§è¡Œæ—¶é—´
                        if task.is_recurring and task.cron_expression and task.scheduled_time < datetime.now():
                            try:
                                trigger = get_cron_trigger(task.cron_expression)
                                next_run = trigger.get_next_fire_time(None, datetime.now())
                                if next_run:
                                    task.scheduled_time = next_run
                                    db.commit()
                                    logger.info(f"é‡å¤ä»»åŠ¡ {task.id} çš„æ‰§è¡Œæ—¶é—´å·²è¿‡æœŸï¼Œå·²æ›´æ–°ä¸ºä¸‹ä¸€æ¬¡æ‰§è¡Œæ—¶é—´: {next_run}")
                            except Exception as e:
                                logger.warning(f"é‡å¤ä»»åŠ¡ {task.id} æ›´æ–°ä¸‹ä¸€æ¬¡æ‰§è¡Œæ—¶é—´å¤±è´¥: {str(e)}")

                        self.add_task(task)
                    except Exception as e:
                        logger.error(f"åŠ è½½ä»»åŠ¡ {task.id} å¤±è´¥: {str(e)}")
                        continue

                logger.info("å¾…å‘é€ä»»åŠ¡åŠ è½½å®Œæˆ")

            except Exception as e:
                logger.error(f"åŠ è½½å¾…å‘é€ä»»åŠ¡å¤±è´¥: {str(e)}")
    
    def get_scheduled_jobs(self):
        """è·å–æ‰€æœ‰å·²è°ƒåº¦çš„ä»»åŠ¡"""
        jobs = []
        for job in self.scheduler.get_jobs():
            jobs.append({
                'id': job.id,
                'next_run_time': job.next_run_time.isoformat() if job.next_run_time else None,
                'trigger': str(job.trigger)
            })
        return jobs
    
    def shutdown(self):
        """å…³é—­è°ƒåº¦å™¨"""
        self.scheduler.shutdown()
        logger.info("é€šçŸ¥è°ƒåº¦å™¨å·²å…³é—­")

    def add_external_calendar_sync_job(self):
        """æ·»åŠ å¤–éƒ¨æ—¥å†åŒæ­¥å®šæ—¶ä»»åŠ¡"""
        if not self.scheduler.get_job('sync_external_calendars'):
            self.scheduler.add_job(
                sync_all_external_calendars,
                'interval',
                minutes=15,
                id='sync_external_calendars',
                replace_existing=True,
                coalesce=True,
                max_instances=1
            )
            logger.info("å¤–éƒ¨æ—¥å†åŒæ­¥ä»»åŠ¡å·²å¯åŠ¨ (æ¯15åˆ†é’Ÿ)")


# å…¨å±€è°ƒåº¦å™¨å®ä¾‹
scheduler = NotifyScheduler()

# --- å¤–éƒ¨æ—¥å†åŒæ­¥é€»è¾‘ ---

def parse_ics_content(content):
    """ç®€æ˜“ ICS è§£æå™¨ (é¿å…å¼•å…¥ heavy ä¾èµ–)"""
    events = []
    lines = content.replace('\r\n', '\n').split('\n')
    
    # å¤„ç†æŠ˜è¡Œ (line unfolding)
    unfolded_lines = []
    for line in lines:
        if line.startswith(' ') or line.startswith('\t'):
            if unfolded_lines:
                unfolded_lines[-1] += line[1:]
        else:
            unfolded_lines.append(line)
            
    current_event = {}
    in_event = False
    
    for line in unfolded_lines:
        if line == 'BEGIN:VEVENT':
            in_event = True
            current_event = {}
        elif line == 'END:VEVENT':
            in_event = False
            if 'DTSTART' in current_event and 'SUMMARY' in current_event:
                events.append(current_event)
        elif in_event:
            if ':' in line:
                key, val = line.split(':', 1)
                # å¤„ç†å‚æ•° (å¦‚ DTSTART;TZID=...)
                prop_name = key.split(';')[0]
                current_event[prop_name] = val
                
    return events

def parse_ics_date(date_str):
    """è§£æ ICS æ—¥æœŸå­—ç¬¦ä¸²"""
    try:
        # æ ¼å¼: 20230101T120000Z æˆ– 20230101T120000
        clean_str = date_str.replace('Z', '')
        if len(clean_str) == 8: # ä»…æ—¥æœŸ
            return datetime.strptime(clean_str, '%Y%m%d')
        return datetime.strptime(clean_str, '%Y%m%dT%H%M%S')
    except Exception:
        return None

def sync_single_calendar(cal_id):
    """åŒæ­¥å•ä¸ªå¤–éƒ¨æ—¥å†"""
    with get_db() as db:
        try:
            cal = db.query(ExternalCalendar).filter(ExternalCalendar.id == cal_id).first()
            if not cal or not cal.is_active:
                return

            logger.info(f"å¼€å§‹åŒæ­¥æ—¥å†: {cal.name} ({cal.url})")
            
            # è·å–é»˜è®¤æ¸ é“é…ç½®
            channel_config = "{}"
            channel_type = "email" # é»˜è®¤ fallback
            if cal.channel_id:
                channel = db.query(UserChannel).filter(UserChannel.id == cal.channel_id).first()
                if channel:
                    channel_config = channel.channel_config
                    channel_type = channel.channel_type
            
            # ä¸‹è½½ ICS
            resp = requests.get(cal.url, timeout=30)
            resp.raise_for_status()
            
            events = parse_ics_content(resp.text)
            count = 0
            
            for event in events:
                uid = event.get('UID')
                if not uid:
                    continue
                    
                ext_uid = f"ext-{cal.id}-{uid}"
                summary = event.get('SUMMARY', 'æ— æ ‡é¢˜')
                desc = event.get('DESCRIPTION', '')
                dt_start_str = event.get('DTSTART')
                
                dt_start = parse_ics_date(dt_start_str)
                if not dt_start or dt_start < datetime.now():
                    continue # è·³è¿‡è¿‡å»çš„ä»»åŠ¡
                
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                existing = db.query(NotifyTask).filter(NotifyTask.external_uid == ext_uid).first()
                
                if existing:
                    # æ›´æ–°
                    if existing.scheduled_time != dt_start or existing.title != summary:
                        existing.scheduled_time = dt_start
                        existing.title = summary
                        existing.content = desc or summary
                        # å¦‚æœä»»åŠ¡ä¹‹å‰å·²å‘é€æˆ–å–æ¶ˆï¼Œé‡æ–°æ¿€æ´»
                        if existing.status in [NotifyStatus.SENT, NotifyStatus.CANCELLED]:
                            existing.status = NotifyStatus.PENDING
                        scheduler.add_task(existing)
                        count += 1
                else:
                    # åˆ›å»ºæ–°ä»»åŠ¡
                    new_task = NotifyTask(
                        user_id=cal.user_id,
                        title=summary,
                        content=desc or summary,
                        channel=channel_type,
                        channel_config=channel_config,
                        scheduled_time=dt_start,
                        status=NotifyStatus.PENDING,
                        external_uid=ext_uid,
                        is_recurring=False # å¤–éƒ¨æ—¥å†çš„é‡å¤ç”±å¤–éƒ¨å¤„ç†ï¼Œè¿™é‡ŒåªåŒæ­¥å…·ä½“äº‹ä»¶
                    )
                    db.add(new_task)
                    db.commit() # æäº¤ä»¥è·å– ID
                    scheduler.add_task(new_task)
                    count += 1
            
            cal.last_sync = datetime.now()
            db.commit()
            logger.info(f"æ—¥å† {cal.name} åŒæ­¥å®Œæˆï¼Œæ›´æ–°/åˆ›å»º {count} ä¸ªä»»åŠ¡")
            
            # é€šçŸ¥å‰ç«¯
            event_manager.announce(cal.user_id, {
                'type': 'calendar_synced',
                'message': f'æ—¥å† "{cal.name}" åŒæ­¥å®Œæˆ'
            })
            
        except Exception as e:
            logger.error(f"åŒæ­¥æ—¥å† {cal_id} å¤±è´¥: {str(e)}")

def sync_all_external_calendars():
    """åŒæ­¥æ‰€æœ‰æ´»è·ƒçš„å¤–éƒ¨æ—¥å†"""
    with get_db() as db:
        cals = db.query(ExternalCalendar).filter(ExternalCalendar.is_active == True).all()
        for cal in cals:
            # ä¸ºæ¯ä¸ªæ—¥å†åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„ job ç«‹å³æ‰§è¡Œï¼Œé¿å…é˜»å¡
            scheduler.scheduler.add_job(
                sync_single_calendar,
                args=[cal.id],
                id=f"sync_cal_{cal.id}_auto",
                replace_existing=True,
                coalesce=True,
                max_instances=1,
                misfire_grace_time=60
            )
